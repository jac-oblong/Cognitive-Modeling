# Homework Assignment 4

## Problem 1 - True-False

### Direct $K$-fold cross-validation requires $K$ model re-fits, which may be computationally demanding, especially when inverse inference is costly

### Bayes factors (BFs) are *relative measures*, that is, they cannot differentiate between "equally good" and "equally bad" models.

### Marginal likelihoods and, by extension, Bayes factors (BFs) cannot be used to compare models with different likelihoods.

### Both the Binomial and the Dirichlet distribution can be formulated as special cases of the Multinomial distribution.

### Bayesian leav-one-out cross-validation (LOO-CV) relies on the posterior predictive distribution of left-out data points.

### The Akaike Information Criterion (AIC) penalizes model complexity indirectly through the variance of a model's marginal likelihood.

### The log-predictive density (LPD) is a relative metric of model complexity.

### The LPD can be approximated by evaluating the likelihood of each posterior draw (e.g., as provided by an MCMC sampler) and taking the average of all resulting likelihood values.

### Bayes factors do not depend on the prior odds, that is, the ratio of prior model probabilities $p(M_1)/p(M_2)$.

## Problem 2 - Simple Multinomial Processing Trees (MPTs)

## Problem 3 - Multiple Regression

The answer to this problem can be found in [problem3.ipynb](./problem3.ipynb).

## Problem 4 - Predictive Distribution

The answer to this problem can be found in [problem4.ipynb](./problem4.ipynb).

## Problem 5 - Reflection

## Problem 6 - Project Pre-Study
